# -*- coding: utf-8 -*-
"""Master-file

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bJxVeviJwdmcjooY9cJjPz_FshvMA4pb
"""

import numpy as np;
import pandas as pd;
from sklearn.linear_model import LogisticRegression
from numpy import mean
from numpy import std

from sklearn.preprocessing import StandardScaler

from sklearn.pipeline import Pipeline
from sklearn import discriminant_analysis
from sklearn.ensemble import AdaBoostClassifier

from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier

# ANOVA feature selection for numeric input and categorical output
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

from sklearn.ensemble import RandomForestClassifier

print("Enter Data file pats: ")
data_file_paths = input()

#read training dataset 1
data1 = pd.read_csv(data_file_paths+'/Dataset_1_Training.csv')

X_train_actual_1=data1[:-2].T[1:].values

y_train_actual_1 = data1[-2:].T[-130:].T.values

#read training dataset 2
data2 = pd.read_csv(data_file_paths+'/Dataset_2_Training.csv')

X_train_actual_2=data2[:-4].T[1:].values

y_train_actual_2 = data2[-4:].T[-340:].T.values

#read testing dataset 1
data_test_1 = pd.read_csv(data_file_paths+'/Dataset_1_Testing.csv')

X_test_1 = data_test_1.T[1:].values

print(X_test_1.shape)

#read testing dataset 2
data_test_2 = pd.read_csv(data_file_paths+'/Dataset_2_Testing.csv')

X_test_2 = data_test_2.T[1:].values

print(X_test_2.shape)

co1={'X':X_train_actual_1,'Y': y_train_actual_1[0]}
co2={'X':X_train_actual_1,'Y': y_train_actual_1[1]}

co3={'X':X_train_actual_2,'Y': y_train_actual_2[0]}
co4={'X':X_train_actual_2,'Y': y_train_actual_2[1]}
co5={'X':X_train_actual_2,'Y': y_train_actual_2[2]}
co6={'X':X_train_actual_2,'Y': y_train_actual_2[3]}

def determine_number_of_top_features(X,Y):
  X=pd.DataFrame(X)
  X=X.astype(float)

  Y=pd.DataFrame(Y)
  Y=Y.astype(int)

  sel = f_classif(X, Y)
  p_values = pd.Series(sel[1])
  p_values.index = X.columns
  p_values.sort_values(ascending=True, inplace=True)
  p_values = p_values[p_values < 0.05]
  X_p = X[p_values.index]
  return X_p.shape[1]

best_co1=determine_number_of_top_features(**co1)
best_co2=determine_number_of_top_features(**co2)
best_co3=determine_number_of_top_features(**co3)
best_co4=determine_number_of_top_features(**co4)
best_co5=determine_number_of_top_features(**co5)
best_co6=determine_number_of_top_features(**co6)

def feature_selector(X,Y,numberOfFeaturesToBeSelected=1):
  if(numberOfFeaturesToBeSelected == 1):
    totalNumberOfFeatures=X.shape[1]
    numberOfFeaturesToBeSelected=totalNumberOfFeatures//2
  # define feature selection
  fs = SelectKBest(score_func=f_classif, k=numberOfFeaturesToBeSelected)
  # apply feature selection
  return fs.fit(X, Y)

def simple_default_ADABoost_fit_predict(X,Y,lr,X_test):
    # define model and parameters
    base_estimator = DecisionTreeClassifier(max_depth=3,class_weight='balanced')
    model = AdaBoostClassifier(base_estimator=base_estimator,learning_rate=lr,n_estimators=50,random_state=1)

    pipeline = Pipeline(
      [
        ('preprocessing',StandardScaler()),
        ('model',model)
      ]
    )
    
    #Fit the classifier
    pipeline.fit(X,Y.astype('int'))

    y_test_pred=pipeline.predict(X_test)

    return y_test_pred

def adaboost_with_Logbase_fit_predict(X,Y,X_test):
    # define model and parameters
    base_estimator = LogisticRegression(class_weight='balanced')
    model = AdaBoostClassifier(base_estimator=base_estimator, algorithm="SAMME", n_estimators=10, learning_rate=0.06, random_state=121)

    pipeline = Pipeline(
      [
        ('preprocessing',StandardScaler()),
        ('model',model)
      ]
    )

    #Fit the classifier
    pipeline.fit(X,Y.astype('int'))

    y_test_pred=pipeline.predict(X_test)

    return y_test_pred

def logisticRegClassifier_pure_model(X,Y,C,penalty,solver,prefit_feature_model):
    # define model
    model = LogisticRegression(C=C,penalty=penalty,solver=solver,class_weight='balanced')
    
    # select features from input X from the supplied prefit model
    X_selected_features = prefit_feature_model.transform(X)
    #Fit the classifier on selected features
    model.fit(X_selected_features,Y.astype('int'))
    return model

def feature_selection_logRegClassification_predict(X,Y,C,penalty,solver,X_test,number_of_features=1):
    # get fitted feature selector model
    prefit_feature_model = feature_selector(X,Y,numberOfFeaturesToBeSelected=number_of_features);
    cModel = logisticRegClassifier_pure_model(X,Y,C,penalty,solver,prefit_feature_model)
    # When training was done on few features, prediction also has to be done by filtering these features
    X_test_selected_features = prefit_feature_model.transform(X_test)
    y_test_pred=cModel.predict(X_test_selected_features)
    return y_test_pred

def LDA_pure_model(X,Y,shrinkage,solver,prefit_feature_model):
    # define model
    model = discriminant_analysis.LinearDiscriminantAnalysis(shrinkage= shrinkage, solver= solver)
    
    # select features from input X from the supplied prefit model
    X_selected_features = prefit_feature_model.transform(X)
    #Fit the classifier on selected features
    model.fit(X_selected_features,Y.astype('int'))
    return model

def feature_selection_LDA_predict(X,Y,shrinkage,solver,X_test,number_of_features=1):
    # get fitted feature selector model
    prefit_feature_model = feature_selector(X,Y,numberOfFeaturesToBeSelected=number_of_features);
    cModel = LDA_pure_model(X,Y,shrinkage,solver,prefit_feature_model)
    # When training was done on few features, prediction also has to be done by filtering these features
    X_test_selected_features = prefit_feature_model.transform(X_test)
    y_test_pred=cModel.predict(X_test_selected_features)
    return y_test_pred

def predict_all_endpoints():
  master_y_pred_out = np.array([])

  # Here LDA and Log-reg showed exact same performance in kaggle
  print("CO1-pred started")
  pred = feature_selection_LDA_predict(co1['X'],co1['Y'],'auto','lsqr',X_test_1,best_co1)
  master_y_pred_out=np.append(master_y_pred_out,pred)

  print("CO2-pred started")
  pred = feature_selection_logRegClassification_predict(co2['X'],co2['Y'],0.1, 'l1','liblinear',X_test_1)
  master_y_pred_out=np.append(master_y_pred_out,pred)
  
  print("CO3-pred started")
  pred = adaboost_with_Logbase_fit_predict(co3['X'],co3['Y'],X_test_2)
  master_y_pred_out=np.append(master_y_pred_out,pred)

  print("CO4-pred started")
  pred = simple_default_ADABoost_fit_predict(co4['X'],co4['Y'],0.1,X_test_2)
  master_y_pred_out=np.append(master_y_pred_out,pred)

  print("CO5-pred started")
  pred = simple_default_ADABoost_fit_predict(co5['X'],co5['Y'],0.1,X_test_2)
  master_y_pred_out=np.append(master_y_pred_out,pred)

  print("CO6-pred started")
  pred = simple_default_ADABoost_fit_predict(co6['X'],co6['Y'],0.1,X_test_2)
  master_y_pred_out=np.append(master_y_pred_out,pred)

  return master_y_pred_out

# After generating csv file, please remove # from "#Id" in the first row of csv
def predict_all_endpoints_output_to_csv():
  total_pred = predict_all_endpoints()

  np.savetxt("prediction-CO1-LDA-CO2-LR-CO3-ADALog-CO4-6-ADBDef-reproduce-2.csv", np.dstack((np.arange(0, total_pred.size),total_pred))[0],"%d,%d",header="Id,Predicted")

"""**After generating csv file, please remove # from "#Id" in the first row of csv**"""
print("**After generating csv file, please remove # from #Id in the first row of csv**")
predict_all_endpoints_output_to_csv()

